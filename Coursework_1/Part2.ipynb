{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Coursework1 Part 2\n",
    "\n",
    "Import libraries required for data processing (pandas, numpy), natural language processing (nltk) and machine learning (sklearn), as well as related analytics functions.\n",
    "\n",
    "Dealing with the BBC News dataset, three features were chosen to train the classification model from, namely feature extraction from full text using bag-of-words model, feature extraction from headlines, and full text extraction using TF-IDF model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54be610966abd16"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vertigo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/vertigo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/vertigo/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vertigo/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:37:55.281665Z",
     "start_time": "2024-03-13T02:37:53.794771Z"
    }
   },
   "id": "29b4968443662166",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "A function that loads a dataset from a bbc directory, where each subdirectory represents a category. Extracts the contents of all files ending in txt, removes line breaks and stores them in the first column of the DataFrame, and stores the category numbers in the second column."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f64e94bada68e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(directory):\n",
    "    data = []\n",
    "    category_dict = {}\n",
    "    # Iterate through the subdirectories\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_dir():\n",
    "            # Create a category dictionary\n",
    "            category_dict[entry.name] = len(category_dict)\n",
    "            # Iterate through the files in the subdirectory\n",
    "            for sub_entry in os.scandir(entry):\n",
    "                if sub_entry.is_file() and sub_entry.name.endswith('.txt'):\n",
    "                    # Open the file and read its content\n",
    "                    with open(sub_entry, 'r', encoding='latin1') as file:\n",
    "                        # Append the content and category\n",
    "                        data.append([file.read().splitlines(), category_dict[entry.name]])\n",
    "                        file.close()\n",
    "    return pd.DataFrame(data, columns=[\"content\", \"category\"]), category_dict  # Return the data and category dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:37:55.286012Z",
     "start_time": "2024-03-13T02:37:55.282882Z"
    }
   },
   "id": "9a0e587745cd09b1",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Load the dataset and split it into training, development and test sets using 80%/10%/10% split ratio for subsequent model training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "898634a892a62178"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_full, category_dict = load_data('bbc')\n",
    "# split train/test/dev data into 80%/10%/10%\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_full['content'], dataset_full['category'], test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:38:02.077974Z",
     "start_time": "2024-03-13T02:37:55.287030Z"
    }
   },
   "id": "e4c524393d28a0f1",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction\n",
    "\n",
    "The bag-of-words feature model for text data preprocessing, which includes functions such as stem extraction, normalisation (conversion to lower case), deletion of deactivated words, and vector representation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d37955073bfa7d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"-\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"''\")\n",
    "stopwords.add(\"'\")\n",
    "stopwords.add(\"(\")\n",
    "stopwords.add(\")\")\n",
    "stopwords.add(\"%\")\n",
    "stopwords.add(\"$\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\";\")\n",
    "stopwords.add(\"'s\")\n",
    "\n",
    "\n",
    "# Convert articles to text tokens\n",
    "def get_list_tokens(pd_array, title=False):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    list_tokens = []\n",
    "    if title:\n",
    "        sentence_split = nltk.tokenize.sent_tokenize(pd_array[0])\n",
    "        for sentence in sentence_split:\n",
    "            list_tokens_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "            for token in list_tokens_sentence:\n",
    "                # Words in lowercase\n",
    "                list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    else:\n",
    "        for content in pd_array:\n",
    "            sentence_split = nltk.tokenize.sent_tokenize(content)\n",
    "            for sentence in sentence_split:\n",
    "                list_tokens_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "                for token in list_tokens_sentence:\n",
    "                    # Words in lowercase\n",
    "                    list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "\n",
    "# Building a vocabulary\n",
    "def get_vocabulary(training_set, num_features, title=False):\n",
    "    dict_word_frequency = {}\n",
    "    for instance in training_set:\n",
    "        # Get the tokens\n",
    "        sentence_tokens = get_list_tokens(instance, title)\n",
    "        for word in sentence_tokens:\n",
    "            # Skip stopwords\n",
    "            if word in stopwords: continue\n",
    "            # Add the word to the dictionary\n",
    "            if word not in dict_word_frequency:\n",
    "                dict_word_frequency[word] = 1\n",
    "            else:\n",
    "                dict_word_frequency[word] += 1\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "    vocabulary_temp = []\n",
    "    for word, frequency in sorted_list:\n",
    "        # Append the word to the vocabulary\n",
    "        vocabulary_temp.append(word)\n",
    "    return vocabulary_temp\n",
    "\n",
    "\n",
    "# Get the vector representation of the text\n",
    "def get_vector_text(list_vocab, string, title=False):\n",
    "    vector_text = np.zeros(len(list_vocab))\n",
    "    # Get the tokens\n",
    "    list_tokens_string = get_list_tokens(string, title)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            # Count the frequency of the word\n",
    "            vector_text[i] = list_tokens_string.count(word)\n",
    "    return vector_text\n",
    "\n",
    "\n",
    "# Get the vector representation of the data\n",
    "def get_vector_data(x_input, y_input, vocabulary_input, title=False):\n",
    "    x_vector = []\n",
    "    y_vector = []\n",
    "    for i in x_input.index:\n",
    "        # Append the vector representation of the text\n",
    "        x_vector.append(get_vector_text(vocabulary_input, x_input.loc[i], title))\n",
    "        # Append the category\n",
    "        y_vector.append(y_input.loc[i])\n",
    "    return x_vector, y_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:38:02.086277Z",
     "start_time": "2024-03-13T02:38:02.079409Z"
    }
   },
   "id": "8ff1026507bcc3f7",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Training and Validation\n",
    "\n",
    "The model is then trained using the SVM classifier and validated using the development set. The best number of features is selected based on the accuracy of the development set, and the model is tested using the test set. The accuracy, precision, recall, and F1 score are used as evaluation metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54cec0a45c7c90b5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:38:02.099489Z",
     "start_time": "2024-03-13T02:38:02.087293Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature: Word Frequency Feature (Bag of Words)\n",
    "# Set the SVM classifier\n",
    "def set_svm_classifier(x_train_svm, y_train_svm):\n",
    "    svm_clf = sklearn.svm.SVC(kernel=\"linear\", gamma='auto')\n",
    "    svm_clf.fit(np.asarray(x_train_svm), np.asarray(y_train_svm))\n",
    "    return svm_clf\n",
    "\n",
    "# Model Testing\n",
    "def validation(model, vocabulary_val, x_input, y_input, title=False):\n",
    "    # Get the vector representation of the data\n",
    "    x_val, y_val = get_vector_data(x_input, y_input, vocabulary_val, title)\n",
    "    \n",
    "    # Get the predictions\n",
    "    predictions = model.predict(x_val)\n",
    "    y_val = np.asarray(y_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    precision = precision_score(y_val, predictions, average=\"macro\")\n",
    "    recall = recall_score(y_val, predictions, average=\"macro\")\n",
    "    f_score = f1_score(y_val, predictions, average=\"macro\")\n",
    "\n",
    "    return accuracy, precision, recall, f_score\n",
    "\n",
    "# Adjust the number of features\n",
    "def adjust_feature_bow(x_train_input, y_train_input, x_dev_input, y_dev_input, list_num_input, title=False):\n",
    "    best_accuracy_dev = 0.0\n",
    "    for num_features in list_num_input:\n",
    "        # Get the vocabulary\n",
    "        vocabulary_feature = get_vocabulary(x_train_input, num_features, title)\n",
    "        # Get the vector representation of the data\n",
    "        x_train_feature, y_train_feature = get_vector_data(x_train_input, y_train_input, vocabulary_feature, title)\n",
    "        # Set the SVM classifier\n",
    "        svm_model = set_svm_classifier(x_train_feature, y_train_feature)\n",
    "        accuracy, precision, recall, f_score = validation(svm_model, vocabulary_feature, x_dev_input, y_dev_input,\n",
    "                                                          title)\n",
    "        print(\"Accuracy with \" + str(num_features) + \": \" + str(round(accuracy, 3)))\n",
    "        # Get the best accuracy\n",
    "        if accuracy >= best_accuracy_dev:\n",
    "            best_accuracy_dev = accuracy\n",
    "            best_num_features = num_features\n",
    "    print(\"\\n Best accuracy overall in the dev set is \" + str(round(best_accuracy_dev, 3)) + \" with \" + str(\n",
    "        best_num_features) + \" features.\\n\\n\")\n",
    "\n",
    "    return svm_model, vocabulary_feature\n",
    "\n",
    "\n",
    "# Model Testing\n",
    "def test_bow_performance(x_train_input, y_train_input, x_test_input, y_test_input, x_dev_input, y_dev_input,\n",
    "                         list_num_input, title=False):\n",
    "    # Getting the best number of features\n",
    "    model_test, vocabulary_test = adjust_feature_bow(x_train_input, y_train_input, x_dev_input, y_dev_input,\n",
    "                                                     list_num_input, title)\n",
    "    # Testing with Test Sets\n",
    "    accuracy, precision, recall, f_score = validation(model_test, vocabulary_test, x_test_input, y_test_input, title)\n",
    "    print(\"Accuracy: \" + str(round(accuracy, 3)))\n",
    "    print(\"macro-averaged precision: \" + str(round(precision, 3)))\n",
    "    print(\"macro-averaged recall: \" + str(round(recall, 3)))\n",
    "    print(\"macro-averaged F_score: \" + str(round(f_score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model BoW (Full text) Testing \n",
    "\n",
    "Full text feature extraction of articles using bag-of-words models and testing the models. The number of features is adjusted to find the best accuracy, and the model is tested using the test set. The accuracy, precision, recall, and F1 score are used as evaluation metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4f620d57a58bef4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 300: 0.914\n",
      "Accuracy with 400: 0.937\n",
      "Accuracy with 500: 0.946\n",
      "Accuracy with 600: 0.95\n",
      "Accuracy with 700: 0.955\n",
      "Accuracy with 800: 0.946\n",
      "Accuracy with 900: 0.946\n",
      "Accuracy with 1000: 0.941\n",
      "\n",
      " Best accuracy overall in the dev set is 0.955 with 700 features.\n",
      "\n",
      "Accuracy: 0.942\n",
      "macro-averaged precision: 0.938\n",
      "macro-averaged recall: 0.946\n",
      "macro-averaged F_score: 0.941\n"
     ]
    }
   ],
   "source": [
    "# feature: Word Frequency Feature (Full Text)\n",
    "list_num_features = [300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "test_bow_performance(X_train, y_train, X_test, y_test, X_dev, y_dev, list_num_features, title=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:39:20.857722Z",
     "start_time": "2024-03-13T02:38:02.100567Z"
    }
   },
   "id": "3175cdd52e3e7fb",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model BoW (Title) Testing\n",
    "\n",
    "Feature extraction from headlines using bag-of-words models and testing the models. The number of features is adjusted to find the best accuracy, and the model is tested using the test set. The accuracy, precision, recall, and F1 score are used as evaluation metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "668c3f206dbfda85"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 700: 0.73\n",
      "Accuracy with 800: 0.739\n",
      "Accuracy with 900: 0.748\n",
      "Accuracy with 1000: 0.757\n",
      "Accuracy with 1100: 0.761\n",
      "Accuracy with 1200: 0.784\n",
      "Accuracy with 1300: 0.788\n",
      "Accuracy with 1400: 0.793\n",
      "\n",
      " Best accuracy overall in the dev set is 0.793 with 1400 features.\n",
      "\n",
      "\n",
      "Accuracy: 0.78\n",
      "macro-averaged precision: 0.784\n",
      "macro-averaged recall: 0.779\n",
      "macro-averaged F_score: 0.78\n"
     ]
    }
   ],
   "source": [
    "# feature: Word Frequency Feature (Title)\n",
    "list_num_features = [700, 800, 900, 1000, 1100, 1200, 1300, 1400]\n",
    "test_bow_performance(X_train, y_train, X_test, y_test, X_dev, y_dev, list_num_features, title=True) \n",
    "# The title parameter controls the feature extraction range"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:39:27.598114Z",
     "start_time": "2024-03-13T02:39:20.859016Z"
    }
   },
   "id": "e5aa3324e16ca702",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction from Full Text using TF-IDF Model\n",
    "\n",
    "The feature extraction from full text using TF-IDF models and testing the models. The number of features is adjusted to find the best accuracy, and the model is tested using the test set. The accuracy, precision, recall, and F1 score are used as evaluation metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a34deed61ec75e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# feature: TF-IDF\n",
    "def adjust_feature_tfidf(x_train_input, y_train_input, x_dev_input, y_dev_input, list_num_input):\n",
    "    best_accuracy_dev = 0.0\n",
    "    # Convert from Array to String\n",
    "    x_train_joined = x_train_input.apply(' '.join)\n",
    "    x_dev_joined = x_dev_input.apply(' '.join)\n",
    "    # Iterate through the number of features\n",
    "    for num_features in list_num_input:\n",
    "        # Set the TF-IDF vectorizer\n",
    "        tfidf = TfidfVectorizer(max_features=num_features, stop_words='english')\n",
    "        tfidf.fit_transform(x_train_joined)\n",
    "        tfidf_x_train = tfidf.transform(x_train_joined)\n",
    "        # Set the SVM classifier\n",
    "        svm_model = sklearn.svm.SVC(kernel=\"linear\", gamma=\"auto\")\n",
    "        svm_model.fit(tfidf_x_train, y_train_input.to_numpy())\n",
    "        tfidf_x_dev = tfidf.transform(x_dev_joined)\n",
    "        # Get the predictions\n",
    "        predictions_idf = svm_model.predict(tfidf_x_dev)\n",
    "        tfidf_y_dev = y_dev_input.to_numpy()\n",
    "        # Get the accuracy\n",
    "        accuracy = accuracy_score(tfidf_y_dev, predictions_idf)\n",
    "        print(\"Accuracy with \" + str(num_features) + \": \" + str(round(accuracy, 3)))\n",
    "        if accuracy >= best_accuracy_dev:\n",
    "            best_accuracy_dev = accuracy\n",
    "            best_num_features = num_features\n",
    "    print(\"\\n Best accuracy overall in the dev set is \" + str(round(best_accuracy_dev, 3)) + \" with \" + str(\n",
    "        best_num_features) + \" features.\\n\\n\")\n",
    "\n",
    "    return svm_model, tfidf\n",
    "\n",
    "\n",
    "def test_tfidf_performance(x_train_input, y_train_input, x_test_input, y_test_input, x_dev_input, y_dev_input,\n",
    "                           list_num_input):\n",
    "    # Getting the best number of features\n",
    "    model_test, tfidf_test = adjust_feature_tfidf(x_train_input, y_train_input, x_dev_input, y_dev_input,\n",
    "                                                  list_num_input)\n",
    "    x_test_joined = x_test_input.apply(' '.join)\n",
    "    tfidf_x_test = tfidf_test.transform(x_test_joined)\n",
    "    # Testing with Test Sets\n",
    "    predictions_idf = model_test.predict(tfidf_x_test)\n",
    "    tfidf_y_test = y_test_input.to_numpy()\n",
    "    accuracy = accuracy_score(tfidf_y_test, predictions_idf)\n",
    "    precision = precision_score(tfidf_y_test, predictions_idf, average=\"macro\")\n",
    "    recall = recall_score(tfidf_y_test, predictions_idf, average=\"macro\")\n",
    "    f_score = f1_score(tfidf_y_test, predictions_idf, average=\"macro\")\n",
    "    print(\"Accuracy: \" + str(round(accuracy, 3)))\n",
    "    print(\"macro-averaged precision: \" + str(round(precision, 3)))\n",
    "    print(\"macro-averaged recall: \" + str(round(recall, 3)))\n",
    "    print(\"macro-averaged F_score: \" + str(round(f_score, 3)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:39:27.603722Z",
     "start_time": "2024-03-13T02:39:27.599298Z"
    }
   },
   "id": "89e02a4606070323",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model TF-IDF Testing\n",
    "\n",
    "Feature extraction from full text using TF-IDF models and testing the models. The number of features is adjusted to find the best accuracy, and the model is tested using the test set. The accuracy, precision, recall, and F1 score are used as evaluation metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "794432794234a617"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 300: 0.937\n",
      "Accuracy with 400: 0.95\n",
      "Accuracy with 500: 0.955\n",
      "Accuracy with 600: 0.959\n",
      "Accuracy with 700: 0.968\n",
      "Accuracy with 800: 0.968\n",
      "Accuracy with 900: 0.959\n",
      "Accuracy with 1000: 0.959\n",
      "\n",
      " Best accuracy overall in the dev set is 0.968 with 800 features.\n",
      "\n",
      "\n",
      "Accuracy: 0.96\n",
      "macro-averaged precision: 0.958\n",
      "macro-averaged recall: 0.963\n",
      "macro-averaged F_score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# feature: TF-IDF\n",
    "list_num_features = [300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "test_tfidf_performance(X_train, y_train, X_test, y_test, X_dev, y_dev, list_num_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T02:39:33.813886Z",
     "start_time": "2024-03-13T02:39:27.604925Z"
    }
   },
   "id": "7f0f0a58f0aa5809",
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
